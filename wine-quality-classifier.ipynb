{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of Simple Tree, Gradient Boosting Tree, and Random Forest Classifiers in analyzing the quality of white wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some stuff we'll need: Pandas, the classifier modules from scikit-learn, a few utility functions for sample selection and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the dataset that we're working with. Today, we'll be using the white wine component of the wine quality dataset from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-white.csv',sep=';',quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn about this dataset? It looks like a number of different features, and then a target variable called \"quality\", which ranges from 3 to 9. The median and 75th percentile values are both 6 out of 9, so we can determine that this class is unbalanced. Indeed, we see that 5 and 6 make up more than 2/3 of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 7, 8, 4, 3, 9], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4898.000000\n",
       "mean        5.877909\n",
       "std         0.885639\n",
       "min         3.000000\n",
       "25%         5.000000\n",
       "50%         6.000000\n",
       "75%         6.000000\n",
       "max         9.000000\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn that unbalanced ordinal class into an unbalanced binary class, and apply that to the dataset based on the value of the \"quality\" feature. We want to build a classifier for the best of the best wines, so it's fine that the class is unbalanced. After all, this isn't Lake Woebegone, where every wine is above average..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isTasty(quality):\n",
    "    if quality >= 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tasty'] = df['quality'].apply(isTasty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality', 'tasty'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3838\n",
       "1    1060\n",
       "Name: tasty, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tasty'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create training and testing subsets that we'll use to train our classifiers. It is best practice to always try train and test your classifiers on different datasets. Here we'll take one-third of the original population and use it for testing, and the other two-thirds will be used for training the classifiers. Note that we can specify a random_state seed in order to get the same results for the same input data if we want to replicate this experiement later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "target = df['tasty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data,target,test_size = 0.33,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the splitting process produce populations having the right shapes for what we're trying to do? Yes it did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, 11), (1617, 11), (3281,), (1617,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[subset.shape for subset in [data_train,data_test,target_train,target_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use our split population of train and test data to train some classifiers. Today we'll be using a simple decision tree, a Gradient-Boosting classifier, and a Random Forest Classifier, all from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all classifiers that we'll be using today, we'll hold the max tree depth at 5. This prevents over-fitting of individual trees, and also allows us to remove that as a factor in the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleTree = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleTree.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbmTree = GradientBoostingClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmTree.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfTree = RandomForestClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfTree.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifiers are trained, let's evaluate their performance. We can call the precision_recall_fscore_support function from sklearn.metrics to return, well... the Precision, Recall, F-Score, and Support measures for each classifier, comparing the original target data with the predicted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleTreePerformance = precision_recall_fscore_support(target_test,simpleTree.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbmTreePerformance = precision_recall_fscore_support(target_test,gbmTree.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfTreePerformance = precision_recall_fscore_support(target_test,rfTree.predict(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by the precision_recall_fscore_support() function is not the prettiest... but we'll solve that later. As you can see, the function returns the precision, recall, f-score, and support metric for each class. Looking at the support metric, we can see that the class-wise composition of the test population is slightly different than the population as a whole. However, it's not crazy unbalanced, so it will not unduly affect the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.86759327,  0.564     ]),\n",
       " array([ 0.91583012,  0.4378882 ]),\n",
       " array([ 0.89105935,  0.49300699]),\n",
       " array([1295,  322], dtype=int64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleTreePerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.8996337 ,  0.73412698]),\n",
       " array([ 0.94826255,  0.57453416]),\n",
       " array([ 0.92330827,  0.6445993 ]),\n",
       " array([1295,  322], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmTreePerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.85110803,  0.61849711]),\n",
       " array([ 0.94903475,  0.33229814]),\n",
       " array([ 0.89740781,  0.43232323]),\n",
       " array([1295,  322], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfTreePerformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, that's a bit easier to read. We're interested in predicting a positive (1) value for \"tasty\", so we're mostly concerned with the performance of each classifier in predicting the value for that class, which is the second number in each list for each metric (ie, the Simple Tree Classifier found 0.566 Precision for the positive class, and the Random Forest Classifier found .326 Recall for the positive class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each of these metrics, we're looking for a value as close to one (1) as possible. We can see that the Gradient Boosted (GBM) tree generally out-performs the others in correctly classifying tasty wines. The GBM tree also achieved a higher recall for the positive class than the other classifiers. This all being the case, it is clear that the GBM tree classifier was the strongest performer in the cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, Fscore, and Support for each class in simple, gradient boosted, and random forest tree classifiers:\n",
      "\n",
      "Precision:  [ 0.86759327  0.564     ]\n",
      "Recall:  [ 0.91583012  0.4378882 ]\n",
      "Fscore:  [ 0.89105935  0.49300699]\n",
      "Support:  [1295  322] \n",
      "\n",
      "Precision:  [ 0.8996337   0.73412698]\n",
      "Recall:  [ 0.94826255  0.57453416]\n",
      "Fscore:  [ 0.92330827  0.6445993 ]\n",
      "Support:  [1295  322] \n",
      "\n",
      "Precision:  [ 0.85110803  0.61849711]\n",
      "Recall:  [ 0.94903475  0.33229814]\n",
      "Fscore:  [ 0.89740781  0.43232323]\n",
      "Support:  [1295  322] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Precision, Recall, Fscore, and Support for each class in simple, gradient boosted, and random forest tree classifiers:'+'\\n')\n",
    "for treeMethod in [simpleTreePerformance,gbmTreePerformance,rfTreePerformance]:\n",
    "    print('Precision: ',treeMethod[0])\n",
    "    print('Recall: ',treeMethod[1])\n",
    "    print('Fscore: ',treeMethod[2])\n",
    "    print('Support: ',treeMethod[3],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way to conceptualize classifier performance - Confusion Matrices. Here, we compare the quantity of predicted values against each actual value. For example, for the Simple Tree, we see that out of the testing sample, 1187 un-tasty wines were correctly classified, and 141 tasty wines were correctly classified. However, there were also 108 tasty wines that were erroneously classified as un-tasty, and 181 un-tasty wines that were classified as tasty. Gross!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for simple, gradient boosted, and random forest tree classifiers:\n",
      "Simple Tree:\n",
      " [[1186  109]\n",
      " [ 181  141]] \n",
      "\n",
      "Gradient Boosted:\n",
      " [[1228   67]\n",
      " [ 137  185]] \n",
      "\n",
      "Random Forest:\n",
      " [[1229   66]\n",
      " [ 215  107]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for simple, gradient boosted, and random forest tree classifiers:')\n",
    "print('Simple Tree:\\n',confusion_matrix(target_test,simpleTree.predict(data_test)),'\\n')\n",
    "print('Gradient Boosted:\\n',confusion_matrix(target_test,gbmTree.predict(data_test)),'\\n')\n",
    "print('Random Forest:\\n',confusion_matrix(target_test,rfTree.predict(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we know that the GBM tree is our favored classifier for predicting the tastiness of wines, that begs the question: \"what makes a tasty wine?\". Luckily, GBM trees produce interpretable results, so we can call the feature_importances method against the GBM tree object and find out which features play the largest role in predicting tastiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that not all machine learning methods produce interpretable results. For example, the random forest classifier is literally a set of multiple decision trees, so it's not easy to say which features played the largest role in determining the outcome of the classification. Neural Networks and Support Vector Machines (SVM) also suffer from this. That's not to say, however, that it cannot be done using local interpretable model explanations (LIME): https://arxiv.org/pdf/1602.04938v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06244312,  0.09792831,  0.06384643,  0.07786289,  0.08666165,\n",
       "        0.08620248,  0.09452112,  0.12985839,  0.08189272,  0.08317855,\n",
       "        0.13560434])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmTree.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little hard to read, we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see that alcohol content and density play the two largest roles in the decision of the GBM classifier, followed by total sulfur diocide and volatile acidity. On the other hand, fixed acidity and residual sugar appear to be less important factors in determining the tastiness of wine. Perhaps if we were looking at red wines, this observation might be different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances for GBM tree\n",
      "\n",
      "fixed acidity: 0.062443117444274894\n",
      "volatile acidity: 0.0979283052153955\n",
      "citric acid: 0.06384643262654617\n",
      "residual sugar: 0.07786289245391594\n",
      "chlorides: 0.0866616456242658\n",
      "free sulfur dioxide: 0.08620248393448902\n",
      "total sulfur dioxide: 0.09452112190619916\n",
      "density: 0.12985838652978277\n",
      "pH: 0.08189272440970403\n",
      "sulphates: 0.08317855241664664\n",
      "alcohol: 0.13560433743878014\n"
     ]
    }
   ],
   "source": [
    "print('Feature Importances for GBM tree\\n')\n",
    "for importance,feature in zip(gbmTree.feature_importances_,['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']):\n",
    "    print('{}: {}'.format(feature,importance))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
